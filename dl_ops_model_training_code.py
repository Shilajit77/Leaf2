# -*- coding: utf-8 -*-
"""DL_OPS_Model Training_CODE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YR38a0GXrisvK8Fb9UozmErPeimadOJY

# Data Loading and Preprocessing
"""

#! pip install kaggle
#! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d csafrit2/plant-leaves-for-image-classification

!unzip /content/plant-leaves-for-image-classification.zip

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import numpy as np
import cv2
import os
import PIL
import tensorflow as tf
import torch
import torch
import torchvision
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
from torchvision.datasets import MNIST
from torchvision.transforms import ToTensor
from torchvision.utils import make_grid
from torch.utils.data.dataloader import DataLoader
from torch.utils.data import random_split
# %matplotlib inline
import os
import torch
import torchvision
import tarfile
from torchvision.datasets.utils import download_url
from torch.utils.data import random_split
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np 
import warnings
warnings.filterwarnings("ignore")
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import torch
from torchvision import transforms
from torchvision.datasets.folder import ImageFolder
import matplotlib.pyplot as plt
import numpy as np
import cv2
import os
import PIL
import time
from tqdm import tqdm
from timeit import default_timer as timer
import tensorflow as tf
import torch
import warnings
import torch
import torchvision
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
from torchvision.datasets import CIFAR10
from torchvision.transforms import ToTensor
from tqdm import tqdm
from torchvision.utils import make_grid
from torch.utils.data.dataloader import DataLoader
from torch.utils.data import random_split
# %matplotlib inline
import os
import torch
import torchvision
import tarfile
from torchvision.datasets.utils import download_url
from torch.utils.data import random_split
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np 
import torchvision.transforms as transforms
import torch.nn as nn
import torch.nn.functional as F
warnings.filterwarnings("ignore")
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device

import pathlib
data_dir = '/content/Plants_2/train//'
data_dir = pathlib.Path(data_dir)
path = data_dir
path

folder_list = ['Alstonia Scholaris diseased (P2a)','Alstonia Scholaris healthy (P2b)','Arjun diseased (P1a)',
              'Arjun healthy (P1b)','Bael diseased (P4b)','Basil healthy (P8)','Chinar diseased (P11b)','Chinar healthy (P11a)',
              'Gauva diseased (P3b)','Gauva healthy (P3a)','Jamun diseased (P5b)','Jamun healthy (P5a)','Jatropha diseased (P6b)',
              'Jatropha healthy (P6a)','Lemon diseased (P10b)','Lemon healthy (P10a)','Mango diseased (P0b)','Mango healthy (P0a)',
              'Pomegranate diseased (P9b)','Pomegranate healthy (P9a)','Pongamia Pinnata diseased (P7b)','Pongamia Pinnata healthy (P7a)']

x = []
y = []
#data_dir.glob('Alstonia Scholaris diseased (P2a)/*')
i = 1
for folder in folder_list:
    string = folder + '/*'
    images = list(data_dir.glob(string))
    for img in images:
        pic = cv2.resize(cv2.imread(str(img)),(64,64))
        x.append(pic) 
        y.append(folder)
    print(f'Folder {i} complete.\n')
    i = i+1

from sklearn import preprocessing
le = preprocessing.LabelEncoder()
y = le.fit_transform(y)
x = np.array(x,dtype='float32')
x = x/255
plt.imshow(x[0])

height = 64
width = 64

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.30)
x_val, x_test, y_val, y_test = train_test_split(x_test, y_test,test_size=0.66)
print(x_train.shape,x_val.shape,x_test.shape)

pd.DataFrame(le.inverse_transform(y_train)).value_counts()

"""# Oversampling (Smote)"""

from imblearn.over_sampling import SMOTE
x_train = x_train.reshape(2991, 64*64*3)
smote = SMOTE(sampling_strategy = 'all')
x_smote, y_smote = smote.fit_resample(x_train , y_train)
x_smote.shape
#X_smote = x_smote.reshape(10800, 28, 28, 3)

x_smote = x_smote.reshape(5192, 64, 64, 3)

pd.DataFrame(le.inverse_transform(y_smote)).value_counts()

a = torch.tensor(x_smote)
b = torch.tensor(y_smote)
c = torch.tensor(x_test)
d = torch.tensor(y_test)
e = torch.tensor(x_val)
f = torch.tensor(y_val)
d = d.type(torch.LongTensor)
b = b.type(torch.LongTensor)
f = f.type(torch.LongTensor)
a =a.reshape(5192,3,height,width)
c = c.reshape(847,3,height,width)
e = e.reshape(436,3,height,width)
print(a.shape)
print(c.shape)
print(e.shape)
from torch.utils.data import TensorDataset,DataLoader
tensor_data1 = TensorDataset(a,b)
tensor_data2 = TensorDataset(c,d)
tensor_data3 = TensorDataset(e,f)
teacher_loader = DataLoader(tensor_data1,shuffle=1,batch_size=128)
test_loader = DataLoader(tensor_data2,shuffle=1,batch_size=128)
val_loader = DataLoader(tensor_data3,shuffle=1,batch_size=128)

torch.save(teacher_loader,'tensor_data1.pt')
torch.save(test_loader,'tesnsor_data2.pt')
torch.save(val_loader,'tesnsor_data3.pt')

"""# Teacher Traning"""

tmodel = torchvision.models.densenet121()
tmodel.classifier = nn.Linear(in_features=1024, out_features=22, bias=True)
for name, child in tmodel.named_children():
   if name in ['features','avgpool','classifier']:
       print(name + ' is unfrozen')
       for param in child.parameters():
           param.requires_grad = True
   else:
       print(name + ' is frozen')
       for param in child.parameters():
           param.requires_grad = False
optimizer2 = torch.optim.Adadelta(filter(lambda p: p.requires_grad, tmodel.parameters()), lr=0.05)
#optimizer2 = torch.optim.SGD(model.parameters(), lr=0.5)
import time
from timeit import default_timer as timer
tmodel = tmodel.cuda()

train_loss = []
val_los = []
train_acc = []
val_acc = []
for i in range(10):
        l1 = []
        l2 = []
        a1 = []
        a2 = []
        start = timer()
        for img,labels in tqdm(teacher_loader, desc='Training', unit=' unit',leave=False,colour='red'):
            



           # print(end - start)
            
            
            
            img = img.to(device)
            labels = labels.to(device)
            output = tmodel(img)
            probs = F.softmax(output,dim=1)
            max_prob,preds = torch.max(probs,dim=1)
            acc1 = (torch.sum(preds == labels).item() / len(preds))
            
            
            #accu=[]
            a1.append(acc1)
            #gg = np.mean(accu)
            
            loss = nn.CrossEntropyLoss()(output,labels)
            l1.append(loss.item())
            loss.backward()
            optimizer2.step()
            optimizer2.zero_grad()
            
        for a,b in tqdm(val_loader, desc='Training', unit=' unit',leave=False,colour='red'):
                a = a.to(device)
                b = b.to(device)
                rr = tmodel(a)
                val_loss = nn.CrossEntropyLoss()(rr,b)
                l2.append(val_loss.item())
                probs = F.softmax(rr,dim=1)
                max_prob,preds = torch.max(probs,dim=1)
                acc = (torch.sum(preds == b).item() / len(preds))
                a2.append(acc)
        train_loss.append(np.mean(l1))
        val_los.append(np.mean(l2))
        train_acc.append(np.mean(a1))
        val_acc.append(np.mean(a2))
        end = timer()
        time = end - start
        time = time/60
        print(f'Epoch {i+1}/10: Train Loss==> {np.mean(l1):.2f} Val Loss==> {np.mean(l2):.2f} Train_Acc==> {np.mean(a1):.2f} Val_Acc==> {np.mean(a2):.2f}')   
        #print(i+1,loss,acc)

fig = plt.figure(figsize=(12, 5))
fig.add_subplot(1, 2, 1)
  
# showing image
plt.plot(train_loss,label='Training')
plt.plot(val_los,label='Testing')
plt.legend()
plt.title("Loss of Teacher Model")
fig.add_subplot(1, 2, 2)
  
# showing image
plt.plot(train_acc,label='Training')
plt.plot(val_acc,label='Testing')
plt.legend()
plt.title("Accuracy of Teacher Model")
plt.show()

import joblib
joblib.dump(tmodel, 'tmodel1.pkl')

start = timer()
predictions = []
accu = []
l_act = []
i = 0
for images_test,labels_test in test_loader:
    #i = i+1```````````````````````````````
    images_test = images_test.to(device)
    labels_test = labels_test.to(device)
    ypred = tmodel(images_test)
    probs = F.softmax(ypred,dim=1)
    max_prob,preds = torch.max(probs,dim=1)
    #print(preds)
    #print(max_prob)
    acc = (torch.sum(preds == labels_test).item() / len(preds))
    accu.append(acc)
    predictions.append(preds)
    l_act.append(labels_test)
    
print(f'Accuracy of Teacher(Densenet121) is : {np.mean(accu):.2f}')
predictions = [t.cpu().numpy() for t in predictions]
predictions = np.array(np.concatenate(predictions))
pd.DataFrame(predictions).value_counts()
ytrue = [t.cpu().numpy() for t in l_act]
ytrue = np.array(np.concatenate(ytrue))
pd.DataFrame(ytrue).value_counts()
report = pd.DataFrame()
report['Actual'] = ytrue
report['Predicted'] = predictions
report['Actual1'] = le.inverse_transform(report['Actual'])
report['Predicted1'] = le.inverse_transform(report['Predicted'])
report5 = report.copy()
from sklearn.metrics import classification_report
print(classification_report(report['Actual1'], report['Predicted1']))

sdata1,sdata2 = random_split(tensor_data1,[5000,192])
sdata1,sdata2 = random_split(sdata1,[2500,2500])

sloader1 = DataLoader((sdata1),shuffle=1,batch_size=64)
sloader2 = DataLoader((sdata2),shuffle=1,batch_size=64)

model3 = models.shufflenet_v2_x2_0(pretrained=False)
model3.fc = nn.Linear(in_features=2048, out_features=22, bias=True)
optimizers = torch.optim.SGD(model3.parameters(), lr=0.5)

train_loss1 = []
val_los1 = []
train_acc1 = []
val_acc1 = []
for i in range(100):
        l1 = []
        l2 = []
        a1 = []
        a2 = []
        start = timer()
        for img,labels in tqdm(sloader2, desc='Training', unit=' unit',leave=False,colour='red'):
            



           # print(end - start)
            
            
            
            img = img.to(device)
            labels = labels.to(device)
            
            output = model3(img)
            #print(output.shape)
            probs = F.softmax(output,dim=1)
            max_prob,preds = torch.max(probs,dim=1)
            #print(len(labels))
            acc1 = (torch.sum(preds == labels).item() / len(preds))
            compare = tmodel(img)
            pdist = F.softmax(compare/4,dim=1)  ##Teacher distribution
            max_prob1,preds1 = torch.max(pdist,dim=1)   #Teacher output
            #accu=[]
            a1.append(acc1)
            #gg = np.mean(accu)
            soft_loss_teacher = nn.KLDivLoss(reduction='batchmean')(F.log_softmax(output /4, dim=1), pdist)
            loss = nn.CrossEntropyLoss()(output,labels)
            loss = 0.4*1*loss + 0.6*1*soft_loss_teacher
            l1.append(loss.item())
            loss.backward()
            optimizers.step()
            optimizers.zero_grad()
            
        for a,b in tqdm(val_loader, desc='Training', unit=' unit',leave=False,colour='red'):
                a = a.to(device)
                b = b.to(device)
                rr = model3(a)
                val_loss = nn.CrossEntropyLoss()(rr,b)
                l2.append(val_loss.item())
                probs = F.softmax(rr,dim=1)
                max_prob,preds = torch.max(probs,dim=1)
                acc = (torch.sum(preds == b).item() / len(preds))
                a2.append(acc)
        train_loss1.append(np.mean(l1))
        val_los1.append(np.mean(l2))
        train_acc1.append(np.mean(a1))
        val_acc1.append(np.mean(a2))
        end = timer()
        time = end - start
        time = time/60
        print(f'Epoch {i+1}/100: Train Loss==> {np.mean(l1):.2f} Val Loss==> {np.mean(l2):.2f} Train_Acc==> {np.mean(a1):.2f} Val_Acc==> {np.mean(a2):.2f}')   
        #print(i+1,loss,acc)

model2 = models.mobilenet_v2(pretrained=False)
model2.classifier = nn.Linear(in_features=1280, out_features=22, bias=True)
optimizers = torch.optim.SGD(model2.parameters(), lr=0.5)

train_loss1 = []
val_los1 = []
train_acc1 = []
val_acc1 = []
for i in range(100):
        l1 = []
        l2 = []
        a1 = []
        a2 = []
        start = timer()
        for img,labels in tqdm(sloader2, desc='Training', unit=' unit',leave=False,colour='red'):
            



           # print(end - start)
            
            
            
            img = img.to(device)
            labels = labels.to(device)
            
            output = model2(img)
            #print(output.shape)
            probs = F.softmax(output,dim=1)
            max_prob,preds = torch.max(probs,dim=1)
            #print(len(labels))
            acc1 = (torch.sum(preds == labels).item() / len(preds))
            compare = tmodel(img)
            pdist = F.softmax(compare/4,dim=1)  ##Teacher distribution
            max_prob1,preds1 = torch.max(pdist,dim=1)   #Teacher output
            #accu=[]
            a1.append(acc1)
            #gg = np.mean(accu)
            soft_loss_teacher = nn.KLDivLoss(reduction='batchmean')(F.log_softmax(output /4, dim=1), pdist)
            loss = nn.CrossEntropyLoss()(output,labels)
            loss = 0.4*1*loss + 0.6*1*soft_loss_teacher
            l1.append(loss.item())
            loss.backward()
            optimizers.step()
            optimizers.zero_grad()
            
        for a,b in tqdm(val_loader, desc='Training', unit=' unit',leave=False,colour='red'):
                a = a.to(device)
                b = b.to(device)
                rr = model2(a)
                val_loss = nn.CrossEntropyLoss()(rr,b)
                l2.append(val_loss.item())
                probs = F.softmax(rr,dim=1)
                max_prob,preds = torch.max(probs,dim=1)
                acc = (torch.sum(preds == b).item() / len(preds))
                a2.append(acc)
        train_loss1.append(np.mean(l1))
        val_los1.append(np.mean(l2))
        train_acc1.append(np.mean(a1))
        val_acc1.append(np.mean(a2))
        end = timer()
        time = end - start
        time = time/60
        print(f'Epoch {i+1}/100: Train Loss==> {np.mean(l1):.2f} Val Loss==> {np.mean(l2):.2f} Train_Acc==> {np.mean(a1):.2f} Val_Acc==> {np.mean(a2):.2f}')   
        #print(i+1,loss,acc)